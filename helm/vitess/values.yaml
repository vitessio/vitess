# This file contains default values for vitess.
#
# You can override these defaults when installing:
#   helm install -f site-values.yaml .
#
# The contents of site-values.yaml will be merged into this default config.
# It's not necessary to copy the defaults into site-values.yaml.
#
# For command-line flag maps like backupFlags or extraFlags,
# use 'flag_name: true|false' to enable or disable a boolean flag.

# The main topology map declares what resources should be created.
# Values for each component (etcd, vtctld, ...) that are not specified here
# will be taken from defaults defined below.
# topology:

# config will be stored as a ConfigMap and mounted where appropriate
config:
  # Backup flags will be applied to components that need them.
  # These are defined globally since all components should agree.
  backup:

    enabled: false

    # this creates 1 cron job per shard that will execute a backup using vtctlclient
    # on this schedule. The job itself uses almost no resources.
    cron:
      # the default schedule runs daily at midnight unless overridden by the individual shard
      schedule: "0 0 * * *"

      # if this is set to true, the cron jobs are created, but never execute
      suspend: false

    # choose a backup service - valid values are gcs/s3
    # TODO: add file and ceph support
    backup_storage_implementation: gcs

    #########
    # gcs settings
    #########

    # Google Cloud Storage bucket to use for backups
    gcs_backup_storage_bucket: vitess-backups

    # root prefix for all backup-related object names
    gcs_backup_storage_root: vtbackups

    # secret that contains Google service account json with read/write access to the bucket
    # kubectl create secret generic vitess-backups-creds --from-file=gcp-creds.json
    # can be omitted if running on a GCE/GKE node with default permissions
    gcsSecret: vitess-gcs-creds

    #########
    # s3 settings
    #########

    # AWS region to use
    s3_backup_aws_region: us-east-1

    # S3 bucket to use for backups
    s3_backup_storage_bucket: vitess-backups

    # root prefix for all backup-related object names
    s3_backup_storage_root: vtbackups

    # server-side encryption algorithm (e.g., AES256, aws:kms)
    s3_backup_server_side_encryption: AES256

    # secret that contains AWS S3 credentials file with read/write access to the bucket
    # kubectl create secret generic s3-credentials --from-file=s3-creds
    # can be omitted if running on a node with default permissions
    s3Secret: vitess-s3-creds

topology:
  globalCell:
    etcd:
      replicas: 3
  cells:
    - name: zone1

      # set failure-domain.beta.kubernetes.io/region
      # region: eastus

      ## You MUST set up at least one keyspace, as these are what actually define and
      #  create the Vitess cluster
      #
      # keyspaces:
      #   - name: "commerce"
      #     shards:
      #       - name: "0"
      #         tablets:
      #           - type: "replica"
      #             vttablet:
      #               replicas: 2

      ##    this defines named jobs that will run vtctlclient ApplySchema to initialize
      #     your tables
      #
      #     schema:
      #       phase1: |-
      #         create table product(
      #           sku varbinary(128),
      #           description varbinary(128),
      #           price bigint,
      #           primary key(sku)
      #         );
      #         create table customer(
      #           user_id bigint not null auto_increment,
      #           email varbinary(128),
      #           primary key(user_id)
      #         );
      #         create table corder(
      #           order_id bigint not null auto_increment,
      #           user_id bigint,
      #           product_id bigint,
      #           msrp bigint,
      #           primary key(order_id)
      #         );

      ##    this defines named jobs that will run vtctlclient ApplyVSchema
      #
      #     vschema:
      #       phase1: |-
      #         {
      #           "tables": {
      #             "product": {},
      #             "customer": {},
      #             "corder": {}
      #           }
      #         }

          ## this defines keyspace specific information for PMM
          # pmm:
            ## PMM supports collecting metrics from custom SQL queries in a file named queries-mysqld.yml
            #  The specified ConfigMap will be mounted in a directory, so the file name is important.
            #  https://www.percona.com/blog/2018/10/10/percona-monitoring-and-management-pmm-1-15-0-is-now-available/
            # config: pmm-commerce-config

      # enable or disable mysql protocol support, with accompanying auth details
      mysqlProtocol:
        enabled: false
        authType: secret
        # authType can be: none or secret. For secret, perform the following changes:
        # username: myuser
        # this is the secret that will be mounted as the user password
        # kubectl create secret generic myuser-password --from-literal=password=abc123
        # passwordSecret: myuser-password

      etcd:
        replicas: 3
      vtctld:
        replicas: 1
      vtgate:
        replicas: 3
        # if maxReplicas is higher than replicas, an HPA will be created
        # maxReplicas: 6

jobs:
  # examples:
  #- name: "split-clone-customer"
  #  kind: "vtworker"
  #  cell: "zone1"
  #  command: "SplitClone customer/0"
  #- name: "list-all-tablet-zone1"
  #  kind: "vtctlclient"
  #  command: "ListAllTablets zone1"

# Default values for etcd resources defined in 'topology'
etcd:
  version: "3.3.10"
  replicas: 3
  resources:
    requests:
      cpu: 200m
      memory: 100Mi
  # If clusterWide is set to true, will add an annotation to EtcdCluster
  # to make this cluster managed by clusterwide operators
  # clusterWide: true

# Default values for vtctld resources defined in 'topology'
vtctld:
  serviceType: ClusterIP
  vitessTag: helm-1.0.7-5
  resources:
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  extraFlags: {}
  secrets: [] # secrets are mounted under /vt/usersecrets/{secretname}

# Default values for vtgate resources defined in 'topology'
vtgate:
  serviceType: ClusterIP
  vitessTag: helm-1.0.7-5
  resources:
    # requests:
    #   cpu: 500m
    #   memory: 512Mi

  # Additional flags that will be appended to the vtgate command.
  # The options below are the most commonly adjusted, but any flag can be put here.
  # run vtgate --help to see all available flags
  extraFlags:
    # MySQL server version to advertise. (default "5.7.9-Vitess")
    # If running 8.0, you may prefer to use something like "8.0.13-Vitess"
    # to prevent db clients from running deprecated queries on startup
    mysql_server_version: "5.7.9-Vitess"

  secrets: [] # secrets are mounted under /vt/usersecrets/{secretname}

# Default values for vtctlclient resources defined in 'topology'
vtctlclient:
  vitessTag: helm-1.0.7-5
  extraFlags: {}
  secrets: [] # secrets are mounted under /vt/usersecrets/{secretname}

# Default values for vtworker resources defined in 'jobs'
vtworker:
  vitessTag: helm-1.0.7-5
  extraFlags: {}
  resources:
    # requests:
    #   cpu: 500m
    #   memory: 512Mi
  secrets: [] # secrets are mounted under /vt/usersecrets/{secretname}


# Default values for vttablet resources defined in 'topology'
vttablet:
  vitessTag: helm-1.0.7-5

  # valid values are
  # - mysql56 (for MySQL 8.0)
  # - mysql56 (for MySQL/Percona 5.6 or 5.7)
  # - mariadb (for MariaDB <= 10.2)
  # - mariadb103 (for MariaDB >= 10.3)
  # the flavor determines the base my.cnf file for vitess to function
  flavor: mysql56

  mysqlImage: percona:5.7.26
  # mysqlImage: mysql:5.7.24
  # mysqlImage: mariadb:10.3.11

  enableHeartbeat: false

  # This requires at least 2 instances of "replica" tablet types, otherwise semi-sync
  # will block forever. "rdonly" tablets do not ACK.
  enableSemisync: false

  # This sets the vttablet flag "-init_db_name_override" to be the keyspace name, rather
  # than "vt_keyspace". This works better with many MySQL client applications
  useKeyspaceNameAsDbName: true

  # The name of a config map with N files inside of it. Each file will be added
  # to $EXTRA_MY_CNF, overriding any default my.cnf settings
  extraMyCnf: ""
  # extraMyCnf: extra-my-cnf

  # mysqlSize can be "test" or "prod". Default is "prod".
  # If the value is "test", then mysql is instanitated with a smaller footprint.
  mysqlSize: prod

  # Additional flags that will be appended to the vttablet command.
  # The options below are the most commonly adjusted, but any flag can be put here.
  # run vttablet --help to see all available flags
  extraFlags:
    # query server max result size, maximum number of rows allowed to return
    # from vttablet for non-streaming queries.
    queryserver-config-max-result-size: 10000

    # query server query timeout (in seconds), this is the query timeout in vttablet side.
    # If a query takes more than this timeout, it will be killed.
    queryserver-config-query-timeout: 30

    # query server connection pool size, connection pool is used by
    # regular queries (non streaming, not in a transaction)
    queryserver-config-pool-size: 24

    # query server stream connection pool size, stream pool is used by stream queries:
    # queries that return results to client in a streaming fashion
    queryserver-config-stream-pool-size: 100

    # query server transaction cap is the maximum number of transactions allowed to
    # happen at any given point of a time for a single vttablet.
    # e.g. by setting transaction cap to 100, there are at most 100 transactions
    # will be processed by a vttablet and the 101th transaction will be blocked
    # (and fail if it cannot get connection within specified timeout)
    queryserver-config-transaction-cap: 300

    # Size of the connection pool for app connections
    app_pool_size: 40

    # Size of the connection pool for dba connections
    dba_pool_size: 20

  # User secrets that will be mounted under /vt/usersecrets/{secretname}/
  secrets: []

  resources:
    # common production values 2-4CPU/4-8Gi RAM
    # requests:
    #   cpu: 2
    #   memory: 4Gi

  mysqlResources:
    # common production values 4CPU/8-16Gi RAM
    # requests:
    #   cpu: 4
    #   memory: 8Gi

  # PVC for mysql
  dataVolumeClaimAnnotations:
  dataVolumeClaimSpec:
    # pd-ssd (Google Cloud)
    # managed-premium (Azure)
    # standard (AWS) - not sure what the default class is for ssd
    # Note: Leave storageClassName unset to use cluster-specific default class.
    #storageClassName: pd-ssd
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 10Gi

# Default values for pmm
pmm:
  enabled: false
  pmmTag: 1.17.0
  client:
    resources:
      requests:
        cpu: 50m
        memory: 128Mi

  server:
    resources:
      # requests:
      #   cpu: 500m
      #   memory: 1Gi

    # PVC for pmm
    dataVolumeClaimAnnotations:
    dataVolumeClaimSpec:
      # storageClassName: pd-ssd
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
    env:
      # DISABLE_TELEMETRY
      # With telemetry enabled, your PMM Server sends some statistics to v.percona.com every 24 hours
      disableTelemetry: true

      # METRICS_RESOLUTION (Option)
      # This option sets the minimum resolution for checking metrics. You should set it if the latency is higher than 1 second
      metricsResolution: 1s

      # METRICS_RETENTION (Option)
      # This option determines how long metrics are stored at PMM Server.
      # The value is passed as a combination of hours, minutes, and seconds, such as 720h0m0s.
      # The minutes (a number followed by m) and seconds (a number followed by s) are optional.
      metricsRetention: 720h

      # QUERIES_RETENTION
      # This option determines how many days queries are stored at PMM Server
      queriesRetention: 8

      # METRICS_MEMORY (Option) -- TODO: automatically calculate based on resource limits
      # NOTE: The value must be passed in kilobytes
      # NOTE: Make sure to quote this value so it isn't converted into scientific notation

      # By default, Prometheus in PMM Server uses up to 768 MB of memory for storing the most recently used data chunks.
      # Depending on the amount of data coming into Prometheus, you may require a higher limit to avoid throttling data ingestion,
      # or allow less memory consumption if it is needed for other processes.
      # The limit affects only memory reserved for data chunks. Actual RAM usage by Prometheus is higher.
      # It is recommended to set this limit to roughly 2/3 of the total memory that you are planning to allow for Prometheus.
      metricsMemory: "600000"

# Orchestrator requires at least version >= 3.0.9 and Kubernetes 1.9 to work
# Default values for orchestrator resources
orchestrator:
  enabled: false
  image: vitess/orchestrator:3.1.1
  replicas: 3
  resources:
    requests:
      cpu: 50m
      memory: 350Mi
